{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJk_OOude0EN"
      },
      "source": [
        "# 환경설정 (for Colab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pvKkDhKUif5n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "433b6941-2888-45da-aec6-bf0f37aeaae5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# 드라이브 마운트\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 작업 디렉토리 경로 변경\n",
        "import os\n",
        "notebook_path = '/content/drive/MyDrive/NewJJeans/HAND'\n",
        "os.chdir(notebook_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mecab 다운로드"
      ],
      "metadata": {
        "id": "_9VdeUbMo9KR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EHZE-aeAak-6",
        "outputId": "82e75955-e0f2-4369-b7d8-cc9a6bdaf633"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/NewJJeans/HAND/mecab/Mecab-ko-for-Google-Colab\n"
          ]
        }
      ],
      "source": [
        "%cd mecab/Mecab-ko-for-Google-Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pnej8x3Uapyy"
      },
      "outputs": [],
      "source": [
        "!bash install_mecab-ko_on_colab_light_220429.sh # 1~3분정도 소요"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "suEwbc_-4mfw",
        "outputId": "4da262ab-7197-45bc-f692-a0eee93bb4c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['‘', '김', '태리', '’', '라', '붐', '소연', ',', '‘', '놀', '면', '뭐', '하', '니', '?', '’', 'WSG', '워너비', '최종', '발탁', '.', '라', '붐', '(', 'LABOUM', ')', '소연', '이', 'WSG', '워너비', '멤버', '로', '합류', '했', '다', '.', '지난', '28', '일', '방송', '된', 'MBC', '‘', '놀', '면', '뭐', '하', '니', '?', '’', '에서', 'WSG', '워너비', '최종', '멤버', '가', '공개', '된', '가운데', ',', '소연', '은', '최종', '12', '인', '에', '이름', '을', '올리', '며', 'WSG', '워너비', '로', '활동', '하', '게', '됐', '다', '.', '앞서', 'WSG', '워너비', '블라인드', '오디션', '에서', '김', '태리', '로', '출연', '한', '소연', '은', '정승환', '의', '‘', '너', '였', '다면', '’', '을', '열창', '해', '3', '사', '대표', '들', '에게', '모두', '합격', '점', '을', '받', '는가', '하', '면', ',', '엘레나', '킴', '은', '“', '우리', '팀', '의', '리더', '가', '돼', '달', '라', '”', '라며', '소연', '을', '향한', '러브', '콜', '을', '보냈', '다', '.', '또한', '전지현', ',', '제시카', '알바', '와', '함께', '‘', '신선봉', '’', '조', '로', '뭉쳤', '던', '소연', '은', '에코', '의', '‘', '행복', '한', '나', '를', '’', '열창', ',', '라', '붐', '의', '메인', '보컬', '답', '게', '청아', '한', '보이스', '와', '완벽', '한', '가창력', '은', '물론', ',', '조원', '들', '과', '환상', '의', '하모니', '를', '선보여', '다음', '라운드', '로', '진출', '했', '다', '.', '독보', '적', '인', '보이스', '와', '가창력', '을', '뽐낸', '김', '태리', '의', '정체', '를', '향한', '여러', '가지', '추측', '과', '관심', '이', '쏟아진', '가운데', ',', '28', '일', '방송', '된', '‘', '놀', '면', '뭐', '하', '니', '?', '’', '에서', '김', '태리', '의', '정체', '가', '소연', '으로', '밝혀', '지', '면서', '모두', '를', '놀라', '게', '했', '다', '.', '소연', '은', 'WSG', '워너비', '로', '합류', '하', '게', '된', '소감', '으로', '“', '예전', '에', 'MSG', '워너비', '가', '부른', '‘', '상상더하기', '’', '를', '보', '고', '너무', '놀라', '서', '멤버', '들', '끼리', '얼싸안', '았', '다', '.', '그', '프로젝트', '에', '제', '가', '들어왔', '다는', '게', '믿기', '지', '않', '는다', '”', '라고', '밝혔', '다', '.', '소연', '이', '앞', '으로', 'WSG', '워너비', '활동', '을', '통해', '어떤', '행보', '를', '걸', '을지', '관심', '이', '집중', '된다', '.', '한편', '소연', '은', '최근', '국방', 'TV', '‘', '리얼', '병영', '톡', '!', '행', '군기', '’', '의', '고정', 'MC', '로', '발탁', '되', '면서', '앞', '으로', '도', '다채', '로운', '활동', '으로', '대중', '을', '만날', '예정', '이', '다', '.']\n"
          ]
        }
      ],
      "source": [
        "from konlpy.tag import Mecab\n",
        "# 안되면 위에 **디렉토리변경** 한 다음에 하세요~!!!!!!!!!!!!!!\n",
        "mecab = Mecab(dicpath='mecab-ko-dic-2.1.1-20180720')\n",
        "word_list = ['아이즈원','권은비', '피프티 피프티', '침착맨', '투모로우바이투게더', '인스타그램', '미미로즈']\n",
        "news = \"‘김태리’ 라붐 소연, ‘놀면뭐하니?’ WSG워너비 최종 발탁. 라붐(LABOUM) 소연이 WSG워너비 멤버로 합류했다.지난 28일 방송된 MBC ‘놀면 뭐하니?’에서 WSG워너비 최종 멤버가 공개된 가운데, 소연은 최종 12인에 이름을 올리며 WSG워너비로 활동하게 됐다.앞서 WSG워너비 블라인드 오디션에서 김태리로 출연한 소연은 정승환의 ‘너였다면’을 열창해 3사 대표들에게 모두 합격점을 받는가 하면, 엘레나 킴은 “우리 팀의 리더가 돼 달라”라며 소연을 향한 러브콜을 보냈다.또한 전지현, 제시카 알바와 함께 ‘신선봉’ 조로 뭉쳤던 소연은 에코의 ‘행복한 나를’ 열창, 라붐의 메인 보컬답게 청아한 보이스와 완벽한 가창력은 물론, 조원들과 환상의 하모니를 선보여 다음 라운드로 진출했다.독보적인 보이스와 가창력을 뽐낸 김태리의 정체를 향한 여러 가지 추측과 관심이 쏟아진 가운데, 28일 방송된 ‘놀면 뭐하니?’에서 김태리의 정체가 소연으로 밝혀지면서 모두를 놀라게 했다.소연은 WSG워너비로 합류하게 된 소감으로 “예전에 MSG워너비가 부른 ‘상상더하기’를 보고 너무 놀라서 멤버들끼리 얼싸안았다. 그 프로젝트에 제가 들어왔다는 게 믿기지 않는다”라고 밝혔다.소연이 앞으로 WSG워너비 활동을 통해 어떤 행보를 걸을지 관심이 집중된다.한편 소연은 최근 국방TV ‘리얼 병영 톡! 행군기’의 고정 MC로 발탁되면서 앞으로도 다채로운 활동으로 대중을 만날 예정이다.\"\n",
        "tokenizer = mecab.morphs\n",
        "\n",
        "print(tokenizer(news))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MjmegOylmDy9"
      },
      "source": [
        "# 0. Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fhYLbNNBLMvK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1426e8a0-afe6-4c37-a716-0fc09430b091"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import csv\n",
        "csv.field_size_limit(sys.maxsize)\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from sklearn import metrics\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "from torch.utils.data.dataset import Dataset\n",
        "import numpy as np\n",
        "import nltk # default word/sentence tokenizer\n",
        "nltk.download('punkt')\n",
        "from torch.utils.data import DataLoader\n",
        "import shutil\n",
        "# from konlpy.tag import Mecab"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mecab = Mecab(dicpath='mecab-ko-dic-2.1.1-20180720').morphs\n",
        "tokenizer(\"권은비\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GY-P2ZhbZjg9",
        "outputId": "1596051f-d4ce-4124-92b8-5a5afb290c31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['권은비']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T47SongXmG8l"
      },
      "outputs": [],
      "source": [
        "def get_evaluation(y_true, y_prob, list_metrics):\n",
        "    y_pred = np.argmax(y_prob, -1)\n",
        "    output = {}\n",
        "    if 'accuracy' in list_metrics:\n",
        "        output['accuracy'] = metrics.accuracy_score(y_true, y_pred)\n",
        "    if 'loss' in list_metrics:\n",
        "        try:\n",
        "            output['loss'] = metrics.log_loss(y_true, y_prob)\n",
        "        except ValueError:\n",
        "            output['loss'] = -1\n",
        "    if 'confusion_matrix' in list_metrics:\n",
        "        output['confusion_matrix'] = str(metrics.confusion_matrix(y_true, y_pred))\n",
        "    return output\n",
        "\n",
        "def matrix_mul(input, weight, bias=False):\n",
        "    feature_list = []\n",
        "    for feature in input:\n",
        "        feature = torch.mm(feature, weight)\n",
        "        if isinstance(bias, torch.nn.parameter.Parameter):\n",
        "            feature = feature + bias.expand(feature.size()[0], bias.size()[1])\n",
        "        feature = torch.tanh(feature).unsqueeze(0)\n",
        "        feature_list.append(feature)\n",
        "\n",
        "    return torch.cat(feature_list, 0).squeeze()\n",
        "\n",
        "def element_wise_mul(input1, input2):\n",
        "\n",
        "    feature_list = []\n",
        "    for feature_1, feature_2 in zip(input1, input2):\n",
        "        feature_2 = feature_2.unsqueeze(1).expand_as(feature_1)\n",
        "        feature = feature_1 * feature_2\n",
        "        feature_list.append(feature.unsqueeze(0))\n",
        "    output = torch.cat(feature_list, 0)\n",
        "\n",
        "    return torch.sum(output, 0).unsqueeze(0)\n",
        "\n",
        "def get_max_lengths(data_path):\n",
        "    word_length_list = []\n",
        "    sent_length_list = []\n",
        "    with open(data_path) as csv_file:\n",
        "        reader = csv.reader(csv_file, quotechar='\"')\n",
        "        for idx, line in enumerate(reader):\n",
        "            text = \"\"\n",
        "            for tx in line[1:]:\n",
        "                text += tx.lower()\n",
        "                text += \" \"\n",
        "            sent_list = sent_tokenize(text)\n",
        "            sent_length_list.append(len(sent_list))\n",
        "\n",
        "            for sent in sent_list:\n",
        "                word_list = word_tokenize(sent)\n",
        "                word_length_list.append(len(word_list))\n",
        "\n",
        "        sorted_word_length = sorted(word_length_list)\n",
        "        sorted_sent_length = sorted(sent_length_list)\n",
        "\n",
        "    return sorted_word_length[int(0.9*len(sorted_word_length))], sorted_sent_length[int(0.9*len(sorted_sent_length))]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3j9UAungEhl"
      },
      "source": [
        "# 1. Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vLxP6z5AgMW5"
      },
      "outputs": [],
      "source": [
        "class MyDataset(Dataset):\n",
        "    def __init__(self, data_path, dict_path, max_length_sentences=16, max_length_word=64, tokenizer=word_tokenize):\n",
        "        super(MyDataset, self).__init__()\n",
        "\n",
        "        texts, labels = [], []\n",
        "        with open(data_path, encoding='utf-8-sig') as csv_file:\n",
        "            reader = csv.reader(csv_file, quotechar='\"')\n",
        "            for idx, line in enumerate(reader):\n",
        "                text = \"\"\n",
        "                for tx in line[1:]:\n",
        "                    text += tx.lower()\n",
        "                    text += \" \"\n",
        "                label = int(line[0])\n",
        "                texts.append(text)\n",
        "                labels.append(label)\n",
        "\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.dict = pd.read_csv(filepath_or_buffer=dict_path, header=None, sep=\" \", quoting=csv.QUOTE_NONE,\n",
        "                                usecols=[0]).values\n",
        "        self.dict = [word[0] for word in self.dict]\n",
        "        self.max_length_sentences = max_length_sentences\n",
        "        self.max_length_word = max_length_word\n",
        "        self.num_classes = len(set(self.labels)) # classification class num\n",
        "\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        label = self.labels[index]\n",
        "        text = self.texts[index]\n",
        "        document_encode = [\n",
        "            [self.dict.index(word) if word in self.dict else -1 for word in self.tokenizer(sentences)] for sentences\n",
        "            in\n",
        "            sent_tokenize(text=text)] # get dictionary's index\n",
        "\n",
        "        for sentences in document_encode: # padding (to max length word)\n",
        "            if len(sentences) < self.max_length_word:\n",
        "                extended_words = [-1 for _ in range(self.max_length_word - len(sentences))]\n",
        "                sentences.extend(extended_words)\n",
        "\n",
        "        if len(document_encode) < self.max_length_sentences:\n",
        "            extended_sentences = [[-1 for _ in range(self.max_length_word)] for _ in\n",
        "                                  range(self.max_length_sentences - len(document_encode))]\n",
        "            document_encode.extend(extended_sentences)\n",
        "\n",
        "        document_encode = [sentences[:self.max_length_word] for sentences in document_encode][\n",
        "                          :self.max_length_sentences]\n",
        "\n",
        "        document_encode = np.stack(arrays=document_encode, axis=0) # 여러개의 배열을 하나의 배열로 쌓아올림\n",
        "        document_encode += 1 # -1을 0으로 맞추기\n",
        "\n",
        "        return document_encode.astype(np.int64), label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CR5tDjAgIG1"
      },
      "source": [
        "# 2. Word Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CmPQxrdzk4kN"
      },
      "outputs": [],
      "source": [
        "class WordAttNet(nn.Module):\n",
        "    def __init__(self, dictionary_path, hidden_size=50):\n",
        "        super(WordAttNet, self).__init__()\n",
        "        dict = pd.read_csv(filepath_or_buffer=dictionary_path, header=None, sep=\" \",\n",
        "                           quoting=csv.QUOTE_NONE).values[:,1:] # 두번째 열부터 선택\n",
        "        dict_len, embed_size = dict.shape\n",
        "        dict_len += 1\n",
        "\n",
        "        unknown_word = np.zeros((1, embed_size))\n",
        "        # unknown_word 배열과 dict 배열을 합친 후, pytorch tensor로 변환함\n",
        "        dict = torch.from_numpy(np.concatenate([unknown_word, dict], axis=0).astype(float))\n",
        "\n",
        "        self.word_weight = nn.Parameter(torch.Tensor(2 * hidden_size, 2 * hidden_size))\n",
        "        self.word_bias = nn.Parameter(torch.Tensor(1, 2 * hidden_size))\n",
        "        self.context_weight = nn.Parameter(torch.Tensor(2 * hidden_size, 1))\n",
        "\n",
        "        self.lookup = nn.Embedding(num_embeddings=dict_len, embedding_dim=embed_size).from_pretrained(dict)\n",
        "\n",
        "\n",
        "        self.gru = nn.GRU(embed_size, hidden_size, bidirectional=True)\n",
        "        self._create_weights(mean=0.0, std=0.05)\n",
        "\n",
        "    def _create_weights(self, mean=0.0, std=0.05):\n",
        "\n",
        "        self.word_weight.data.normal_(mean, std)\n",
        "        self.context_weight.data.normal_(mean, std)\n",
        "\n",
        "    def forward(self, input, hidden_state):\n",
        "\n",
        "        output = self.lookup(input)\n",
        "        f_output, h_output = self.gru(output.float(), hidden_state)  # feature output and hidden state output\n",
        "        output = matrix_mul(f_output, self.word_weight, self.word_bias)\n",
        "        output = matrix_mul(output, self.context_weight).permute(1,0)\n",
        "        output = F.softmax(output, dim=-1)\n",
        "        output = element_wise_mul(f_output,output.permute(1,0))\n",
        "\n",
        "        return output, h_output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l98t_4xzgJ3E"
      },
      "source": [
        "# 3. Sentence Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AsZtz3h2k-jN"
      },
      "outputs": [],
      "source": [
        "class SentAttNet(nn.Module):\n",
        "    def __init__(self, sent_hidden_size=50, word_hidden_size=50, num_classes=8):\n",
        "        super(SentAttNet, self).__init__()\n",
        "\n",
        "        self.sent_weight = nn.Parameter(torch.Tensor(2 * sent_hidden_size, 2 * sent_hidden_size))\n",
        "        self.sent_bias = nn.Parameter(torch.Tensor(1, 2 * sent_hidden_size))\n",
        "        self.context_weight = nn.Parameter(torch.Tensor(2 * sent_hidden_size, 1))\n",
        "\n",
        "        self.gru = nn.GRU(2 * word_hidden_size, sent_hidden_size, bidirectional=True)\n",
        "        self.fc = nn.Linear(2 * sent_hidden_size, num_classes)\n",
        "\n",
        "        self._create_weights(mean=0.0, std=0.05)\n",
        "\n",
        "    def _create_weights(self, mean=0.0, std=0.05):\n",
        "        self.sent_weight.data.normal_(mean, std)\n",
        "        self.context_weight.data.normal_(mean, std)\n",
        "\n",
        "    def forward(self, input, hidden_state):\n",
        "\n",
        "        f_output, h_output = self.gru(input, hidden_state)\n",
        "        output = matrix_mul(f_output, self.sent_weight, self.sent_bias)\n",
        "        output = matrix_mul(output, self.context_weight).permute(1, 0)\n",
        "        output = F.softmax(output, dim=-1)\n",
        "        output = element_wise_mul(f_output, output.permute(1, 0)).squeeze(0)\n",
        "        output = self.fc(output)\n",
        "\n",
        "        return output, h_output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PgHPnwamlKod"
      },
      "source": [
        "# 4. Hierarchical Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C_btT6kilOwH"
      },
      "outputs": [],
      "source": [
        "class HierAttNet(nn.Module):\n",
        "    def __init__(self, word_hidden_size, sent_hidden_size, batch_size, num_classes, pretrained_dictionary_path,\n",
        "                 max_sent_length, max_word_length):\n",
        "        super(HierAttNet, self).__init__()\n",
        "        self.batch_size = batch_size\n",
        "        self.word_hidden_size = word_hidden_size\n",
        "        self.sent_hidden_size = sent_hidden_size\n",
        "        self.max_sent_length = max_sent_length\n",
        "        self.max_word_length = max_word_length\n",
        "        self.word_att_net = WordAttNet(pretrained_dictionary_path, word_hidden_size)\n",
        "        self.sent_att_net = SentAttNet(sent_hidden_size, word_hidden_size, num_classes)\n",
        "        self._init_hidden_state()\n",
        "\n",
        "    def _init_hidden_state(self, last_batch_size=None):\n",
        "        if last_batch_size:\n",
        "            batch_size = last_batch_size\n",
        "        else:\n",
        "            batch_size = self.batch_size\n",
        "        self.word_hidden_state = torch.zeros(2, batch_size, self.word_hidden_size)\n",
        "        self.sent_hidden_state = torch.zeros(2, batch_size, self.sent_hidden_size)\n",
        "        if torch.cuda.is_available():\n",
        "            self.word_hidden_state = self.word_hidden_state.cuda()\n",
        "            self.sent_hidden_state = self.sent_hidden_state.cuda()\n",
        "\n",
        "    def forward(self, input):\n",
        "\n",
        "        output_list = []\n",
        "        input = input.permute(1, 0, 2)\n",
        "        for i in input:\n",
        "            output, self.word_hidden_state = self.word_att_net(i.permute(1, 0), self.word_hidden_state)\n",
        "            output_list.append(output)\n",
        "        output = torch.cat(output_list, 0)\n",
        "        output, self.sent_hidden_state = self.sent_att_net(output, self.sent_hidden_state)\n",
        "\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "696CloJPh5aZ"
      },
      "source": [
        "# parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "njK1dk4qh5aZ"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "num_epochs = 20\n",
        "learning_rate = 1e-3\n",
        "####################################################################################\n",
        "word_hidden_size = 64\n",
        "sent_hidden_size = 64\n",
        "####################################################################################\n",
        "# train_data = 'data/Training_dataset.txt' # training data\n",
        "train_data = 'data/new_training_0823.txt'\n",
        "test_data = 'data/new_validation_0823.txt'\n",
        "####################################################################################\n",
        "tokenizer = Mecab(dicpath='mecab-ko-dic-2.1.1-20180720').nouns\n",
        "dictionary_path = 'data/ko_w2v_version2.txt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E2vDnHSSh5aa"
      },
      "outputs": [],
      "source": [
        "training_params = {\"batch_size\": batch_size, \"shuffle\": True, \"drop_last\": True}\n",
        "test_params = {'batch_size': batch_size, 'shuffle':False, \"drop_last\":False}\n",
        "max_word_length, max_sent_length = 32, 20 #get_max_lengths(train_data) # 32 20 정도 하면 될듯 ..??\n",
        "training_set = MyDataset(train_data, dictionary_path, max_sent_length, max_word_length, tokenizer=tokenizer)\n",
        "training_generator = DataLoader(training_set, **training_params)\n",
        "test_set = MyDataset(test_data, dictionary_path, max_sent_length, max_word_length, tokenizer=tokenizer)\n",
        "test_generator = DataLoader(test_set, **test_params)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_word_length, max_sent_length"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MHlp3SsMZ320",
        "outputId": "322c28b2-6415-4013-d265-00378339c713"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(30, 21)"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWprX0LLm_Bq"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ymRxYc6Pe0EW"
      },
      "outputs": [],
      "source": [
        "model = HierAttNet(\n",
        "    word_hidden_size, sent_hidden_size,\n",
        "    batch_size, training_set.num_classes,\n",
        "    dictionary_path, max_sent_length, max_word_length\n",
        "    )\n",
        "\n",
        "# move model to GPU\n",
        "if torch.cuda.is_available():\n",
        "    model.cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D2lqJwa31_cm"
      },
      "outputs": [],
      "source": [
        "\"\"\" 모델 epoch 평가 \"\"\"\n",
        "def eval_epoch(num_epoch, epoch, model, criterion, optimizer, data_loader):\n",
        "    model.eval() # 모델을 평가 모드로 설정\n",
        "\n",
        "    loss_ls = []\n",
        "    losses = []\n",
        "    te_label_ls = []\n",
        "    te_pred_ls = []\n",
        "    best_loss = 1e5\n",
        "    #tqdm.notebook.tqdm\n",
        "    with tqdm_notebook(total=len(data_loader), desc=f\"Valid\") as pbar:\n",
        "        for feature, label in data_loader:\n",
        "            num_sample = len(label)\n",
        "\n",
        "            if torch.cuda.is_available():\n",
        "                feature = feature.cuda()\n",
        "                label = label.cuda()\n",
        "\n",
        "            with torch.no_grad():\n",
        "                model._init_hidden_state(num_sample)\n",
        "                te_predictions = model(feature)\n",
        "\n",
        "            te_loss = criterion(te_predictions, label)\n",
        "            loss_ls.append(te_loss * num_sample)\n",
        "            te_label_ls.extend(label.clone().cpu())\n",
        "            te_pred_ls.append(te_predictions.clone().cpu())\n",
        "            pbar.update(1)\n",
        "\n",
        "        te_loss = sum(loss_ls) / test_set.__len__()\n",
        "        te_pred = torch.cat(te_pred_ls, 0)\n",
        "        label = np.array(te_label_ls)\n",
        "        test_metrics = get_evaluation(label, te_pred.numpy(), list_metrics=[\"accuracy\", \"confusion_matrix\"])\n",
        "        confusion_matrix_str = test_metrics.get('confusion_matrix', None)\n",
        "\n",
        "        print('============================== Validation ==============================')\n",
        "        print(\"Epoch: {}/{}, Lr: {}, Loss: {}, Accuracy: {}\".format(\n",
        "            epoch + 1,\n",
        "            num_epochs,\n",
        "            optimizer.param_groups[0]['lr'],\n",
        "            te_loss, test_metrics[\"accuracy\"]))\n",
        "        print('============================== Validation ==============================')\n",
        "\n",
        "    return test_metrics[\"accuracy\"], test_metrics[\"confusion_matrix\"], te_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dxwrtluT-zSx"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm, tqdm_notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZynKqNOe0EY"
      },
      "source": [
        "# Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q4nNBx9Jh5ad"
      },
      "outputs": [],
      "source": [
        "# 손실함수와 optimizer를 정의합니다.\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "best_loss = 1e5\n",
        "best_epoch = 0\n",
        "num_iter_per_epoch = len(training_generator)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Resume from trained checkpoint"
      ],
      "metadata": {
        "id": "2TU3Cm6YpKJ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.tensorboard import SummaryWriter\n",
        "logdir = 'data/logs'\n",
        "writer = SummaryWriter(logdir)"
      ],
      "metadata": {
        "id": "2sMzNQJQqubl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RL56YyhVrsjf"
      },
      "outputs": [],
      "source": [
        "model_path = 'data/checkpoints'\n",
        "model_name_template = '/model_epoch{}_acc{}.pth'\n",
        "\n",
        "# 체크포인트 파일 경로\n",
        "checkpoint_path = 'data/checkpointsAfter_epoch5.pth'  # 체크포인트 파일 경로\n",
        "\n",
        "checkpoint = torch.load(checkpoint_path, map_location=torch.device('cpu')) # using CPU\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "start_epoch = checkpoint['epoch'] + 1  # 재개할 에포크 설정\n",
        "\n",
        "# start_epoch  = 0\n",
        "for epoch in range(start_epoch, num_epochs):\n",
        "    params = {'num_epoch': num_epochs, 'epoch': epoch, 'model': model, 'optimizer': optimizer, 'criterion': criterion}\n",
        "    num_iter_per_epoch = len(training_generator)\n",
        "\n",
        "    for iter, (feature, label) in enumerate(training_generator):\n",
        "        model.train() # 모델을 학습 모드로 설정\n",
        "        if torch.cuda.is_available():\n",
        "            feature = feature.cuda()\n",
        "            label = label.cuda()\n",
        "        model._init_hidden_state()\n",
        "        predictions = model(feature)\n",
        "\n",
        "        # 손실 함수를 이용하여 loss를 계산\n",
        "        loss = criterion(predictions, label)\n",
        "\n",
        "        # optimizer의 gradient 초기화\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward() # gradient 계산\n",
        "        optimizer.step() # model's parameter update using optimizer\n",
        "\n",
        "        training_metrics = get_evaluation(label.cpu().numpy(), predictions.cpu().detach().numpy(), list_metrics=[\"accuracy\"])\n",
        "\n",
        "        if (iter+1) % 100 == 0:\n",
        "            print(\"Epoch: {}/{}, Iteration: {}/{}, Lr: {}, Loss: {}, Accuracy: {}\".format(\n",
        "                epoch + 1,\n",
        "                num_epochs,\n",
        "                iter + 1,\n",
        "                num_iter_per_epoch,\n",
        "                optimizer.param_groups[0]['lr'],\n",
        "                loss, training_metrics[\"accuracy\"]))\n",
        "\n",
        "\n",
        "        # 100번째 반복마다 저장 및 validation\n",
        "        if (iter+1) % 400 == 0:\n",
        "            # 모델 및 옵티마이저 상태 저장\n",
        "            val_accuracy, confusion_matrix, te_loss = eval_epoch(**params, data_loader=test_generator)\n",
        "            model_file_name = model_name_template.format(epoch, val_accuracy)\n",
        "            torch.save({'model_state_dict': model.state_dict(), 'optimizer_state_dict': optimizer.state_dict(), 'epoch': epoch}, model_path + model_file_name)\n",
        "            # training 및 validation Loss 저장\n",
        "            writer.add_scalar(\"Train/Loss\", loss, epoch * num_iter_per_epoch + iter)\n",
        "            writer.add_scalar('Train/Accuracy', training_metrics[\"accuracy\"], epoch * num_iter_per_epoch + iter)\n",
        "            writer.add_scalar(\"Validation/Loss\", te_loss, epoch * num_iter_per_epoch + iter)\n",
        "            writer.add_scalar('Validation/Accuracy', val_accuracy, epoch * num_iter_per_epoch + iter)\n",
        "\n",
        "    model_file_name = f'/After_epoch{epoch}.pth'\n",
        "    torch.save({'model_state_dict': model.state_dict(), 'optimizer_state_dict': optimizer.state_dict(), 'epoch': epoch}, model_path + model_file_name)\n",
        "\n",
        "# 학습 종료 후 close 메소드 호출\n",
        "writer.flush()\n",
        "writer.close()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SJx0nwpdv3Mf",
        "outputId": "cab531b2-bc25-44cb-8753-7c73b835d902"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " \u001b[0m\u001b[01;34mcheckpoints\u001b[0m/                   test.ipynb\n",
            "'Copy of train_0822.ipynb'      train_0819.ipynb\n",
            "'Copy of train.ipynb'           train_0820.ipynb\n",
            " \u001b[01;34mdata\u001b[0m/                          train_0822.ipynb\n",
            " \u001b[01;34mmecab\u001b[0m/                         \u001b[01;34mtrained_models\u001b[0m/\n",
            " \u001b[01;34mmecab-ko-dic-2.1.1-20180720\u001b[0m/   Training_0823_WED_newdata.ipynb\n",
            " \u001b[01;34mMecab-ko-for-Google-Colab\u001b[0m/     training_mecab.ipynb\n",
            " \u001b[01;34mold_data\u001b[0m/                      train-mecab.ipynb\n",
            " \u001b[01;34mpredictions\u001b[0m/                   train-mecab_Woojoo.ipynb\n",
            " reduce_vector_dim.ipynb        train-new.ipynb\n",
            " \u001b[01;34msrc\u001b[0m/                           Untitled0.ipynb\n",
            " \u001b[01;34mtensorboard\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir=runs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "payGrQh4vaTW",
        "outputId": "5ea35ed3-6433-4765-d998-80144495f632"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "ERROR: Failed to launch TensorBoard (exited with 1).\n",
              "Contents of stderr:\n",
              "2023-08-22 19:36:45.404673: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
              "\n",
              "NOTE: Using experimental fast data loading logic. To disable, pass\n",
              "    \"--load_fast=false\" and report issues on GitHub. More details:\n",
              "    https://github.com/tensorflow/tensorboard/issues/4784\n",
              "\n",
              "Address already in use\n",
              "Port 6006 is in use by another program. Either identify and stop that program, or start the server with a different port."
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%reload_ext tensorboard"
      ],
      "metadata": {
        "id": "5HTBd1JQw5uN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorboard --logdir {logdir} --port 6000"
      ],
      "metadata": {
        "id": "n7wf9I6rwQG-"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    },
    "vscode": {
      "interpreter": {
        "hash": "f41e1cc7670ea4f36b583d0300139665460618ecf0268b6e88e349fe439dc38f"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}